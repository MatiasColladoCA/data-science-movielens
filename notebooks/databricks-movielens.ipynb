{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcc0a9c4-f6b6-4306-8a7b-f873d940c017",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Vista rapida de datasets con Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b06d20b-58cb-44b9-9526-2e9142f11a10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# El SparkSession ya existe en el notebook de Databricks por defecto\n",
    "csv_files = [\n",
    "    \"genome-scores.csv\",\n",
    "    \"genome-tags.csv\",\n",
    "    \"links.csv\",\n",
    "    \"movies.csv\",\n",
    "    \"ratings.csv\",\n",
    "    \"tags.csv\"\n",
    "]\n",
    "\n",
    "base_path = \"/Volumes/workspace/movie/movielens/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e89e698f-9c06-4ff8-9d24-32e68127803e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear vistas temporales para poder consultarlas con SQL\n",
    "movies_df.createOrReplaceTempView(\"movies_view\")\n",
    "ratings_df.createOrReplaceTempView(\"ratings_view\")\n",
    "\n",
    "# --- Consultas B√°sicas con Spark SQL ---\n",
    "\n",
    "# 1. Ver las primeras 10 pel√≠culas\n",
    "print(\"Primeras 10 pel√≠culas:\")\n",
    "spark.sql(\"SELECT title, genres FROM movies_view LIMIT 10\").show(truncate=False)\n",
    "\n",
    "# 2. Encontrar todas las pel√≠culas de 'Toy Story'\n",
    "print(\"\\nPel√≠culas de 'Toy Story':\")\n",
    "spark.sql(\"SELECT * FROM movies_view WHERE title LIKE 'Toy Story%'\").show(truncate=False)\n",
    "\n",
    "# 3. Ver los ratings m√°s altos (por encima de 4.5)\n",
    "print(\"\\nRatings m√°s altos:\")\n",
    "spark.sql(\"SELECT userId, movieId, rating FROM ratings_view WHERE rating > 4.5 ORDER BY rating DESC\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6b25dba-73ec-44bc-ac68-0a9c9257785b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Carga Optimizada y Definici√≥n de Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e4cb8eb-4e47-4487-9174-004d0b714255",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Definici√≥n de Schemas ---\n",
    "# Evita errores como por ejemplo suma de strings y agiliza la lectura\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, TimestampType\n",
    "\n",
    "# Schema para movies.csv\n",
    "movies_schema = StructType([\n",
    "    StructField(\"movieId\", IntegerType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"genres\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Schema para ratings.csv\n",
    "ratings_schema = StructType([\n",
    "    StructField(\"userId\", IntegerType(), True),\n",
    "    StructField(\"movieId\", IntegerType(), True),\n",
    "    StructField(\"rating\", FloatType(), True),\n",
    "    StructField(\"timestamp\", IntegerType(), True) # Lo leeremos como int y luego transformaremos\n",
    "])\n",
    "\n",
    "# --- Carga de Datos con Schemas ---\n",
    "base_path = \"/Volumes/workspace/movie/movielens/\"\n",
    "\n",
    "# Cargar los datos principales usando los schemas definidos\n",
    "movies_df = spark.read.format(\"csv\").option(\"header\", \"true\").schema(movies_schema).load(base_path + \"movies.csv\")\n",
    "ratings_df = spark.read.format(\"csv\").option(\"header\", \"true\").schema(ratings_schema).load(base_path + \"ratings.csv\")\n",
    "\n",
    "print(\"Datos de pel√≠culas cargados con schema:\")\n",
    "movies_df.printSchema()\n",
    "movies_df.show(5, truncate=False)\n",
    "\n",
    "print(\"\\nDatos de ratings cargados con schema:\")\n",
    "ratings_df.printSchema()\n",
    "ratings_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ad89c93-577c-4a49-8e61-f87b8908ce2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(movies_df.select(\"genres\").distinct())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cc16788-cc5d-4077-bb43-bd3433412dc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Transformaci√≥n y Limpieza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8444eaa-5105-4245-98e1-a7a1128c4183",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Eliminar duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e71317b8-d42e-49b7-9bf1-2b8f56c1bf7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def remove_duplicates(df: DataFrame, subset_cols: list, df_name: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Identifica y elimina filas duplicadas de un DataFrame bas√°ndose en un subconjunto de columnas. La funci√≥n agrega una capa de auditaci√≥n y reporte que dropDuplicates no logra.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): El DataFrame de Spark a limpiar.\n",
    "        subset_cols (list): Lista de nombres de columna para considerar la unicidad.\n",
    "        df_name (str): Nombre descriptivo del DataFrame, usado para el logging.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Un nuevo DataFrame con los duplicados eliminados.\n",
    "    \"\"\"\n",
    "    initial_count = df.count()\n",
    "    unique_count = df.select(subset_cols).distinct().count()\n",
    "    \n",
    "    duplicates_found = initial_count - unique_count\n",
    "    \n",
    "    print(f\"--- Limpieza de Duplicados para: {df_name} ---\")\n",
    "    print(f\"Filas iniciales: {initial_count}\")\n",
    "    print(f\"Filas √∫nicas basadas en {subset_cols}: {unique_count}\")\n",
    "    print(f\"Duplicados a eliminar: {duplicates_found}\\n\")\n",
    "    \n",
    "    if duplicates_found > 0:\n",
    "        df_cleaned = df.dropDuplicates(subset_cols)\n",
    "        print(f\"Duplicados eliminados. Nuevo total de filas: {df_cleaned.count()}\\n\")\n",
    "        return df_cleaned\n",
    "    else:\n",
    "        print(\"No se encontraron duplicados. El DataFrame no fue modificado.\\n\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b47d963e-03ab-4d97-bcdb-09e9114df2c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Limpieza de Duplicados ---\n",
    "# Eliminamos posibles pel√≠culas duplicadas antes de cualquier transformaci√≥n.\n",
    "movies_df_clean = remove_duplicates(df=movies_df, subset_cols=['movieId'], df_name='Movies DataFrame Original')\n",
    "\n",
    "# # Eliminamos posibles pel√≠culas duplicadas antes de cualquier transformaci√≥n.\n",
    "# movies_df_clean = remove_duplicates(df=movies_df, subset_cols=['movieId'], df_name='Movies DataFrame Original')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79507ddb-6c78-410b-95a3-d399c47a5d10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Separaci√≥n de a√±o y generos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3368a7b8-66ed-4424-95a7-8725cfbc1451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, explode, regexp_extract, from_unixtime, to_timestamp, lit, when\n",
    "\n",
    "# --- Transformaci√≥n del DataFrame de Pel√≠culas (Versi√≥n Profundamente Limpia) ---\n",
    "# 1. Extraer el a√±o de forma segura (usando try_cast).\n",
    "# 2. Separar los g√©neros.\n",
    "# 3. Convertir el marcador \"(no genres listed)\" a NULL para consistencia.\n",
    "\n",
    "movies_transformed_df = movies_df_clean.withColumn(\n",
    "    \"year\",\n",
    "    regexp_extract(col(\"title\"), r\"\\((\\d{4})\\)\", 1).try_cast(\"int\")\n",
    ").withColumn(\n",
    "    \"genre\",\n",
    "    explode(split(col(\"genres\"), \"\\\\|\"))\n",
    ").withColumn(\n",
    "    # Reemplazamos el texto de marcador por un verdadero NULL\n",
    "    \"genre\",\n",
    "    when(col(\"genre\") == \"(no genres listed)\", lit(None)).otherwise(col(\"genre\"))\n",
    ").select(\n",
    "    \"movieId\",\n",
    "    \"title\",\n",
    "    \"year\",\n",
    "    \"genre\"\n",
    ")\n",
    "\n",
    "print(\"Pel√≠culas transformadas (g√©neros explotados y valores nulos estandarizados):\")\n",
    "movies_transformed_df.show(10, truncate=False)\n",
    "\n",
    "\n",
    "# --- Transformaci√≥n del DataFrame de Ratings (Versi√≥n Limpia) ---\n",
    "# 1. Convertir el timestamp de Unix Epoch a un formato de fecha legible.\n",
    "# 2. Convertir ratings inv√°lidos (nulos o 0.0) a NULL.\n",
    "\n",
    "ratings_transformed_df = ratings_df.withColumn(\n",
    "    \"rating_timestamp\",\n",
    "    to_timestamp(from_unixtime(col(\"timestamp\")))\n",
    ").withColumn(\n",
    "    # Si el rating es nulo o 0.0, se convierte a NULL. De lo contrario, se mantiene el valor.\n",
    "    \"rating\",\n",
    "    when((col(\"rating\").isNull()) | (col(\"rating\") <= 0.0), lit(None)).otherwise(col(\"rating\"))\n",
    ").select(\n",
    "    \"userId\",\n",
    "    \"movieId\",\n",
    "    \"rating\",\n",
    "    \"rating_timestamp\"\n",
    ")\n",
    "\n",
    "print(\"\\nRatings transformados y limpios:\")\n",
    "ratings_transformed_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5e7ae17-56fa-4a67-a288-c8cb16890bad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Buscar valores √∫nicos para entender y limpiar mejor las columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4df460c8-b243-4bf7-89c5-951c9b211625",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def show_unique_values(df: DataFrame, df_name: str, limit: int = None, ignore_cols: list = []):\n",
    "    \"\"\"\n",
    "    Muestra los valores √∫nicos de cada columna de un DataFrame para su validaci√≥n.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): El DataFrame de Spark a analizar.\n",
    "        df_name (str): El nombre del DataFrame para imprimir en el encabezado.\n",
    "        limit (int, optional): N√∫mero m√°ximo de valores √∫nicos a mostrar por columna.\n",
    "                               Si es None, muestra todos. Por defecto es None.\n",
    "        ignore_cols (list): Lista de nombres de columna que se omitir√°n en el an√°lisis.\n",
    "    \"\"\"\n",
    "    print(f\"--- Validaci√≥n de Valores √önicos para: {df_name} ---\\n\")\n",
    "    \n",
    "    for column_name in df.columns:\n",
    "        if column_name in ignore_cols:\n",
    "            continue\n",
    "        print(f\"Columna: '{column_name}'\")\n",
    "        \n",
    "        if limit is not None:\n",
    "            # Muestra solo el n√∫mero limitado de valores\n",
    "            df.select(column_name).distinct().show(limit, truncate=False)\n",
    "        else:\n",
    "            # Muestra TODOS los valores √∫nicos\n",
    "            # Advertencia: Esto puede ser lento o consumir mucha memoria para columnas con muchos valores √∫nicos.\n",
    "            unique_values = [row[column_name] for row in df.select(column_name).distinct().collect()]\n",
    "            for value in unique_values:\n",
    "                print(value)\n",
    "                \n",
    "        print(\"-\" * 40) # Separador para mayor claridad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "502d0536-c0a5-49e4-9b52-43c80cb87766",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar valores √∫nicos para el DataFrame de pel√≠culas\n",
    "ignore = [\"movieId\", \"title\"]\n",
    "show_unique_values(movies_transformed_df, \"Movies Transformado\", ignore_cols=ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d95bb95-7ea3-4171-b01f-30b01e733056",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar valores √∫nicos para el DataFrame de ratings\n",
    "ignore = [\"rating_timestamp\", \"movieId\", \"userId\"]\n",
    "show_unique_values(ratings_transformed_df, \"Ratings Transformado\", ignore_cols=ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ef75b90-414e-435a-b6fa-c870a806e77b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# --- Paso de Limpieza Avanzada: Eliminar T√≠tulos Colados en G√©neros ---\n",
    "\n",
    "# 1. Primero, inspeccionemos qu√© filas coinciden con nuestro patr√≥n sospechoso.\n",
    "#    Usamos .rlike() para aplicar la expresi√≥n regular.\n",
    "#    La regex busca cualquier string que contenga \"(4 d√≠gitos)\" y luego una comilla doble.\n",
    "print(\"üîç Filas identificadas con un posible t√≠tulo en la columna 'genre':\")\n",
    "suspicious_rows_df = movies_transformed_df.filter(col(\"genre\").rlike('.*\\(\\d{4}\\).*\"'))\n",
    "suspicious_rows_df.show(truncate=False)\n",
    "\n",
    "# 2. Ahora, eliminamos estas filas del DataFrame.\n",
    "#    El s√≠mbolo '~' es el operador \"NOT\", por lo que filtramos para mantener todo lo que NO coincide con el patr√≥n.\n",
    "movies_cleaned_df = movies_transformed_df.filter(~col(\"genre\").rlike('.*\\(\\d{4}\\).*\"'))\n",
    "\n",
    "print(f\"\\n‚úÖ Se han eliminado {suspicious_rows_df.count()} filas incorrectas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ec306a8-802d-4282-b50b-9ffbdc17edaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Buscar valores nulos y borrarlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ab10b5c-e7c8-4308-ad79-aa35fe1ea5b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, when, col\n",
    "\n",
    "total_count_movies = movies_transformed_df.count()\n",
    "\n",
    "# Calcular y mostrar la proporci√≥n de nulos para cada columna de forma unificada\n",
    "movies_transformed_df.select(\n",
    "    [(count(when(col(c).isNull(), c)) / total_count_movies).alias(c) for c in movies_transformed_df.columns]\n",
    ").show()\n",
    "\n",
    "total_count_ratings = ratings_transformed_df.count()\n",
    "\n",
    "# Calcular y mostrar la proporci√≥n de nulos para cada columna\n",
    "ratings_transformed_df.select(\n",
    "    [(count(when(col(c).isNull(), c)) / total_count_movies).alias(c) for c in ratings_transformed_df.columns]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a18a6765-a617-4a25-b963-bd6f82f431c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def drop_nulls_and_report(df: DataFrame, df_name: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Elimina todas las filas que contienen al menos un valor nulo y reporta\n",
    "    la cantidad de filas eliminadas.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): El DataFrame de Spark a limpiar.\n",
    "        df_name (str): Nombre descriptivo del DataFrame para el logging.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Un nuevo DataFrame sin filas nulas.\n",
    "    \"\"\"\n",
    "    initial_count = df.count()\n",
    "    \n",
    "    print(f\"--- Eliminaci√≥n de Nulos para: {df_name} ---\")\n",
    "    print(f\"Filas iniciales: {initial_count}\")\n",
    "    \n",
    "    # na.drop() elimina cualquier fila que contenga un valor nulo en CUALQUIER columna\n",
    "    df_cleaned = df.na.drop()\n",
    "    \n",
    "    final_count = df_cleaned.count()\n",
    "    rows_dropped = initial_count - final_count\n",
    "    \n",
    "    print(f\"Filas eliminadas con nulos: {rows_dropped}\")\n",
    "    print(f\"Filas finales: {final_count}\\n\")\n",
    "    \n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0a829c2-ae70-427b-a50f-caa0cc71cb62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Limpiar el DataFrame de pel√≠culas\n",
    "movies_clean_df = drop_nulls_and_report(movies_transformed_df, \"Movies Transformado\")\n",
    "\n",
    "# Limpiar el DataFrame de ratings\n",
    "ratings_clean_df = drop_nulls_and_report(ratings_transformed_df, \"Ratings Transformado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ef6f559-d6cb-4b9c-9050-ad8331d31797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Agregaci√≥n y Joins simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "706eba9c-9742-45cf-9147-57d925790828",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Agregaciones y Joins Simples ---\n",
    "\n",
    "# Spark SQL: Contar el n√∫mero total de ratings\n",
    "total_ratings_sql = spark.sql(\"SELECT COUNT(*) as total_ratings FROM ratings_view\").collect()[0][0]\n",
    "print(f\"Total de ratings en la tabla (SQL): {total_ratings_sql}\")\n",
    "\n",
    "# DataFrame API: Hacer lo mismo\n",
    "total_ratings_df = ratings_df.count()\n",
    "print(f\"Total de ratings en la tabla (DataFrame API): {total_ratings_df}\")\n",
    "\n",
    "# Spark SQL: Encontrar las 5 pel√≠culas con m√°s ratings\n",
    "print(\"\\nTop 5 pel√≠culas con m√°s ratings (SQL):\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        movieId,\n",
    "        COUNT(*) as num_ratings\n",
    "    FROM ratings_view\n",
    "    GROUP BY movieId\n",
    "    ORDER BY num_ratings DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show()\n",
    "\n",
    "# Join simple con Spark SQL: Unir una pel√≠cula con sus ratings\n",
    "print(\"\\nJoin simple: Ver los ratings para la pel√≠cula 'Jumanji':\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        m.title,\n",
    "        r.userId,\n",
    "        r.rating\n",
    "    FROM movies_view m\n",
    "    JOIN ratings_view r ON m.movieId = r.movieId\n",
    "    WHERE m.title = 'Jumanji (1995)'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60a4c31b-b0c7-4986-b546-3f75f0fd9542",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Guardar en formato Delta Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ceee35c-5686-4d18-9af1-6ef3d8c6308c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Uni√≥n de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f68037a5-9c39-4290-ad31-d1ad0915a73b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Paso 1: Unir los dos DataFrames en una tabla \"Golden\" ---\n",
    "# Usamos un 'inner join' para quedarnos solo con los ratings que tienen una pel√≠cula asociada.\n",
    "\n",
    "# Es una buena pr√°ctica renombrar las columnas antes del join para evitar ambig√ºedades,\n",
    "# aunque en este caso 'movieId' es la √∫nica en com√∫n y es la clave del join.\n",
    "# El join se realizar√° autom√°ticamente en esta columna.\n",
    "\n",
    "golden_df = ratings_transformed_df.join(\n",
    "    movies_transformed_df,\n",
    "    on=\"movieId\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "print(\"Vista previa de la tabla 'Golden' unida y desnormalizada:\")\n",
    "golden_df.printSchema()\n",
    "golden_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8bc6d06-d787-4250-bacc-a50edad40db3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Guardar versi√≥n final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c5290f8-c6c5-46a5-8994-fd2602fcf626",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Paso 2: Guardar la Tabla Golden en formato Delta Lake (Corregido) ---\n",
    "\n",
    "delta_path = \"/Volumes/workspace/movie/movielens/delta_tables/movielens_analyzed_final\"\n",
    "# Particionamos por 'year' porque es una columna de baja cardinalidad\n",
    "# y muy com√∫n para filtrar en an√°lisis de series temporales.\n",
    "(golden_df.write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .partitionBy(\"year\")\n",
    " .option(\"overwriteSchema\", \"true\")\n",
    " .save(delta_path))\n",
    "\n",
    "print(f\"‚úÖ Tabla 'Golden' guardada exitosamente en Delta Lake en: {delta_path}\")\n",
    "\n",
    "# --- Validaci√≥n Final ---\n",
    "print(\"\\nüîç Validaci√≥n: Leyendo la tabla final desde Delta Lake:\")\n",
    "final_df = spark.read.format(\"delta\").load(delta_path)\n",
    "final_df.show(5)\n",
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38dbc977-736a-40ce-ba32-5a087e6a102c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Visualizaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc52e794-6c4b-41c8-813e-845f6c5ed061",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## G√©neros mas populares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b62cd8c-3c57-4cf6-bf1e-64cafca8131c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZnJvbSBweXNwYXJrLnNxbC5mdW5jdGlvbnMgaW1wb3J0IGNvdW50LCBhdmcsIGRlc2MKCiMgQ2FsY3VsYXIgZWwgbsO6bWVybyBkZSByYXRpbmdzIHBvciBnw6luZXJvCmdlbnJlX3BvcHVsYXJpdHlfZGYgPSBnb2xkZW5fZGYuZ3JvdXBCeSgiZ2VucmUiKS5hZ2coY291bnQoInJhdGluZyIpLmFsaWFzKCJudW1fcmF0aW5ncyIpKS5vcmRlckJ5KGRlc2MoIm51bV9yYXRpbmdzIikpCgpwcmludCgiUG9wdWxhcmlkYWQgZGUgbG9zIGfDqW5lcm9zIHBvciBuw7ptZXJvIGRlIHJhdGluZ3M6IikKZ2VucmVfcG9wdWxhcml0eV9kZi5zaG93KCkKCiMgLS0tIFZpc3VhbGl6YWNpw7NuIC0tLQojIEVuIERhdGFicmlja3MsIHNpbXBsZW1lbnRlIHVzYSBkaXNwbGF5KCkgc29icmUgZWwgRGF0YUZyYW1lIHkgc2VsZWNjaW9uYSAiQmFyIENoYXJ0IgpkaXNwbGF5KGdlbnJlX3BvcHVsYXJpdHlfZGYp\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewb03802b\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewb03802b\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewb03802b\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewb03802b) SELECT `genre`,SUM(`num_ratings`) `column_76c86a5a310` FROM q GROUP BY `genre`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewb03802b\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "genre",
             "id": "column_76c86a5a309"
            },
            "y": [
             {
              "column": "num_ratings",
              "id": "column_76c86a5a310",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_76c86a5a310": {
             "name": "num_ratings",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": true,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "e00a6c34-0f53-4ef3-9f7a-7e4ea07abdb4",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 8.75,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "genre",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "genre",
           "type": "column"
          },
          {
           "alias": "column_76c86a5a310",
           "args": [
            {
             "column": "num_ratings",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, avg, desc\n",
    "\n",
    "# Calcular el n√∫mero de ratings por g√©nero\n",
    "genre_popularity_df = golden_df.groupBy(\"genre\").agg(count(\"rating\").alias(\"num_ratings\")).orderBy(desc(\"num_ratings\"))\n",
    "\n",
    "print(\"Popularidad de los g√©neros por n√∫mero de ratings:\")\n",
    "genre_popularity_df.show()\n",
    "\n",
    "# --- Visualizaci√≥n ---\n",
    "display(genre_popularity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0be5f5d8-a267-413a-b568-85905e72ef6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBDYWxjdWxhciBlbCByYXRpbmcgcHJvbWVkaW8gcG9yIGHDsW8KYXZnX3JhdGluZ19ieV95ZWFyX2RmID0gZ29sZGVuX2RmLmZpbHRlcihjb2woInllYXIiKS5pc05vdE51bGwoKSkuZ3JvdXBCeSgieWVhciIpLmFnZyhhdmcoInJhdGluZyIpLmFsaWFzKCJhdmdfcmF0aW5nIikpLm9yZGVyQnkoInllYXIiKQoKcHJpbnQoIlJhdGluZyBwcm9tZWRpbyBkZSBsYXMgcGVsw61jdWxhcyBwb3IgYcOxbyBkZSBsYW56YW1pZW50bzoiKQphdmdfcmF0aW5nX2J5X3llYXJfZGYuc2hvdygyMCkKCiMgLS0tIFZpc3VhbGl6YWNpw7NuIC0tLQojIFVzYSBkaXNwbGF5KCkgeSBzZWxlY2Npb25hICJMaW5lIENoYXJ0IgpkaXNwbGF5KGF2Z19yYXRpbmdfYnlfeWVhcl9kZik=\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewcc8cf2d\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewcc8cf2d\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewcc8cf2d\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewcc8cf2d) SELECT `year`,`avg_rating` FROM q\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewcc8cf2d\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "year",
             "id": "column_76c86a5a319"
            },
            "y": [
             {
              "column": "avg_rating",
              "id": "column_76c86a5a320"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "scatter",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_76c86a5a320": {
             "name": "avg_rating",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "a035832e-0c8d-4c28-9adf-084c60226de2",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 8.875,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "selects": [
          {
           "column": "year",
           "type": "column"
          },
          {
           "column": "avg_rating",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calcular el rating promedio por a√±o\n",
    "avg_rating_by_year_df = golden_df.filter(col(\"year\").isNotNull()).groupBy(\"year\").agg(avg(\"rating\").alias(\"avg_rating\")).orderBy(\"year\")\n",
    "\n",
    "print(\"Rating promedio de las pel√≠culas por a√±o de lanzamiento:\")\n",
    "avg_rating_by_year_df.show(20)\n",
    "\n",
    "# --- Visualizaci√≥n ---\n",
    "display(avg_rating_by_year_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "198b7b94-9ac9-4096-a1d1-b811846e75f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Concusiones\n",
    "\n",
    "A partir del an√°lisis del dataset de MovieLens, se pueden extraer las siguientes conclusiones simples:\n",
    "\n",
    "1.  **Dominancia de G√©neros:** Los g√©neros de **Drama** y **Comedia** son los que concentran la mayor cantidad de ratings, lo que sugiere una fuerte preferencia del p√∫blico por este tipo de contenido.\n",
    "\n",
    "2.  **Calidad vs. Antig√ºedad:** El an√°lisis del rating promedio por a√±o muestra que entre las decadas del 20 y el 80 las pel√≠culas tuvieron un rating algo sostenido a diferencia de epocas anteriores y posteriores."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "databricks-movielens",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
