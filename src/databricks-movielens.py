#!/usr/bin/env python
# coding: utf-8

# # Vista rapida de datasets con Spark SQL

# In[ ]:


# El SparkSession ya existe en el notebook de Databricks por defecto
csv_files = [
    "genome-scores.csv",
    "genome-tags.csv",
    "links.csv",
    "movies.csv",
    "ratings.csv",
    "tags.csv"
]

base_path = "/Volumes/workspace/movie/movielens/"


# In[ ]:


# Crear vistas temporales para poder consultarlas con SQL
movies_df.createOrReplaceTempView("movies_view")
ratings_df.createOrReplaceTempView("ratings_view")

# --- Consultas B√°sicas con Spark SQL ---

# 1. Ver las primeras 10 pel√≠culas
print("Primeras 10 pel√≠culas:")
spark.sql("SELECT title, genres FROM movies_view LIMIT 10").show(truncate=False)

# 2. Encontrar todas las pel√≠culas de 'Toy Story'
print("\nPel√≠culas de 'Toy Story':")
spark.sql("SELECT * FROM movies_view WHERE title LIKE 'Toy Story%'").show(truncate=False)

# 3. Ver los ratings m√°s altos (por encima de 4.5)
print("\nRatings m√°s altos:")
spark.sql("SELECT userId, movieId, rating FROM ratings_view WHERE rating > 4.5 ORDER BY rating DESC").show(10)


# # Carga Optimizada y Definici√≥n de Schema

# In[ ]:


# --- Definici√≥n de Schemas ---
# Evita errores como por ejemplo suma de strings y agiliza la lectura
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, TimestampType

# Schema para movies.csv
movies_schema = StructType([
    StructField("movieId", IntegerType(), True),
    StructField("title", StringType(), True),
    StructField("genres", StringType(), True)
])

# Schema para ratings.csv
ratings_schema = StructType([
    StructField("userId", IntegerType(), True),
    StructField("movieId", IntegerType(), True),
    StructField("rating", FloatType(), True),
    StructField("timestamp", IntegerType(), True) # Lo leeremos como int y luego transformaremos
])

# --- Carga de Datos con Schemas ---
base_path = "/Volumes/workspace/movie/movielens/"

# Cargar los datos principales usando los schemas definidos
movies_df = spark.read.format("csv").option("header", "true").schema(movies_schema).load(base_path + "movies.csv")
ratings_df = spark.read.format("csv").option("header", "true").schema(ratings_schema).load(base_path + "ratings.csv")

print("Datos de pel√≠culas cargados con schema:")
movies_df.printSchema()
movies_df.show(5, truncate=False)

print("\nDatos de ratings cargados con schema:")
ratings_df.printSchema()
ratings_df.show(5, truncate=False)


# In[ ]:


display(movies_df.select("genres").distinct())


# # Transformaci√≥n y Limpieza

# ## Eliminar duplicados

# In[ ]:


from pyspark.sql import DataFrame

def remove_duplicates(df: DataFrame, subset_cols: list, df_name: str) -> DataFrame:
    """
    Identifica y elimina filas duplicadas de un DataFrame bas√°ndose en un subconjunto de columnas. La funci√≥n agrega una capa de auditaci√≥n y reporte que dropDuplicates no logra.

    Args:
        df (DataFrame): El DataFrame de Spark a limpiar.
        subset_cols (list): Lista de nombres de columna para considerar la unicidad.
        df_name (str): Nombre descriptivo del DataFrame, usado para el logging.

    Returns:
        DataFrame: Un nuevo DataFrame con los duplicados eliminados.
    """
    initial_count = df.count()
    unique_count = df.select(subset_cols).distinct().count()
    
    duplicates_found = initial_count - unique_count
    
    print(f"--- Limpieza de Duplicados para: {df_name} ---")
    print(f"Filas iniciales: {initial_count}")
    print(f"Filas √∫nicas basadas en {subset_cols}: {unique_count}")
    print(f"Duplicados a eliminar: {duplicates_found}\n")
    
    if duplicates_found > 0:
        df_cleaned = df.dropDuplicates(subset_cols)
        print(f"Duplicados eliminados. Nuevo total de filas: {df_cleaned.count()}\n")
        return df_cleaned
    else:
        print("No se encontraron duplicados. El DataFrame no fue modificado.\n")
        return df


# In[ ]:


# --- Limpieza de Duplicados ---
# Eliminamos posibles pel√≠culas duplicadas antes de cualquier transformaci√≥n.
movies_df_clean = remove_duplicates(df=movies_df, subset_cols=['movieId'], df_name='Movies DataFrame Original')

# # Eliminamos posibles pel√≠culas duplicadas antes de cualquier transformaci√≥n.
# movies_df_clean = remove_duplicates(df=movies_df, subset_cols=['movieId'], df_name='Movies DataFrame Original')


# ## Separaci√≥n de a√±o y generos

# In[ ]:


from pyspark.sql.functions import col, split, explode, regexp_extract, from_unixtime, to_timestamp, lit, when

# --- Transformaci√≥n del DataFrame de Pel√≠culas (Versi√≥n Profundamente Limpia) ---
# 1. Extraer el a√±o de forma segura (usando try_cast).
# 2. Separar los g√©neros.
# 3. Convertir el marcador "(no genres listed)" a NULL para consistencia.

movies_transformed_df = movies_df_clean.withColumn(
    "year",
    regexp_extract(col("title"), r"\((\d{4})\)", 1).try_cast("int")
).withColumn(
    "genre",
    explode(split(col("genres"), "\\|"))
).withColumn(
    # Reemplazamos el texto de marcador por un verdadero NULL
    "genre",
    when(col("genre") == "(no genres listed)", lit(None)).otherwise(col("genre"))
).select(
    "movieId",
    "title",
    "year",
    "genre"
)

print("Pel√≠culas transformadas (g√©neros explotados y valores nulos estandarizados):")
movies_transformed_df.show(10, truncate=False)


# --- Transformaci√≥n del DataFrame de Ratings (Versi√≥n Limpia) ---
# 1. Convertir el timestamp de Unix Epoch a un formato de fecha legible.
# 2. Convertir ratings inv√°lidos (nulos o 0.0) a NULL.

ratings_transformed_df = ratings_df.withColumn(
    "rating_timestamp",
    to_timestamp(from_unixtime(col("timestamp")))
).withColumn(
    # Si el rating es nulo o 0.0, se convierte a NULL. De lo contrario, se mantiene el valor.
    "rating",
    when((col("rating").isNull()) | (col("rating") <= 0.0), lit(None)).otherwise(col("rating"))
).select(
    "userId",
    "movieId",
    "rating",
    "rating_timestamp"
)

print("\nRatings transformados y limpios:")
ratings_transformed_df.show(10, truncate=False)


# ## Buscar valores √∫nicos para entender y limpiar mejor las columnas

# In[ ]:


from pyspark.sql import DataFrame

def show_unique_values(df: DataFrame, df_name: str, limit: int = None, ignore_cols: list = []):
    """
    Muestra los valores √∫nicos de cada columna de un DataFrame para su validaci√≥n.
    
    Args:
        df (DataFrame): El DataFrame de Spark a analizar.
        df_name (str): El nombre del DataFrame para imprimir en el encabezado.
        limit (int, optional): N√∫mero m√°ximo de valores √∫nicos a mostrar por columna.
                               Si es None, muestra todos. Por defecto es None.
        ignore_cols (list): Lista de nombres de columna que se omitir√°n en el an√°lisis.
    """
    print(f"--- Validaci√≥n de Valores √önicos para: {df_name} ---\n")
    
    for column_name in df.columns:
        if column_name in ignore_cols:
            continue
        print(f"Columna: '{column_name}'")
        
        if limit is not None:
            # Muestra solo el n√∫mero limitado de valores
            df.select(column_name).distinct().show(limit, truncate=False)
        else:
            # Muestra TODOS los valores √∫nicos
            # Advertencia: Esto puede ser lento o consumir mucha memoria para columnas con muchos valores √∫nicos.
            unique_values = [row[column_name] for row in df.select(column_name).distinct().collect()]
            for value in unique_values:
                print(value)
                
        print("-" * 40) # Separador para mayor claridad


# In[ ]:


# Mostrar valores √∫nicos para el DataFrame de pel√≠culas
ignore = ["movieId", "title"]
show_unique_values(movies_transformed_df, "Movies Transformado", ignore_cols=ignore)


# In[ ]:


# Mostrar valores √∫nicos para el DataFrame de ratings
ignore = ["rating_timestamp", "movieId", "userId"]
show_unique_values(ratings_transformed_df, "Ratings Transformado", ignore_cols=ignore)


# In[ ]:


from pyspark.sql.functions import col
# --- Paso de Limpieza Avanzada: Eliminar T√≠tulos Colados en G√©neros ---

# 1. Primero, inspeccionemos qu√© filas coinciden con nuestro patr√≥n sospechoso.
#    Usamos .rlike() para aplicar la expresi√≥n regular.
#    La regex busca cualquier string que contenga "(4 d√≠gitos)" y luego una comilla doble.
print("üîç Filas identificadas con un posible t√≠tulo en la columna 'genre':")
suspicious_rows_df = movies_transformed_df.filter(col("genre").rlike('.*\(\d{4}\).*"'))
suspicious_rows_df.show(truncate=False)

# 2. Ahora, eliminamos estas filas del DataFrame.
#    El s√≠mbolo '~' es el operador "NOT", por lo que filtramos para mantener todo lo que NO coincide con el patr√≥n.
movies_cleaned_df = movies_transformed_df.filter(~col("genre").rlike('.*\(\d{4}\).*"'))

print(f"\n‚úÖ Se han eliminado {suspicious_rows_df.count()} filas incorrectas.")


# ## Buscar valores nulos y borrarlos

# In[ ]:


from pyspark.sql.functions import count, when, col

total_count_movies = movies_transformed_df.count()

# Calcular y mostrar la proporci√≥n de nulos para cada columna de forma unificada
movies_transformed_df.select(
    [(count(when(col(c).isNull(), c)) / total_count_movies).alias(c) for c in movies_transformed_df.columns]
).show()

total_count_ratings = ratings_transformed_df.count()

# Calcular y mostrar la proporci√≥n de nulos para cada columna
ratings_transformed_df.select(
    [(count(when(col(c).isNull(), c)) / total_count_movies).alias(c) for c in ratings_transformed_df.columns]
).show()


# In[ ]:


from pyspark.sql import DataFrame

def drop_nulls_and_report(df: DataFrame, df_name: str) -> DataFrame:
    """
    Elimina todas las filas que contienen al menos un valor nulo y reporta
    la cantidad de filas eliminadas.

    Args:
        df (DataFrame): El DataFrame de Spark a limpiar.
        df_name (str): Nombre descriptivo del DataFrame para el logging.

    Returns:
        DataFrame: Un nuevo DataFrame sin filas nulas.
    """
    initial_count = df.count()
    
    print(f"--- Eliminaci√≥n de Nulos para: {df_name} ---")
    print(f"Filas iniciales: {initial_count}")
    
    # na.drop() elimina cualquier fila que contenga un valor nulo en CUALQUIER columna
    df_cleaned = df.na.drop()
    
    final_count = df_cleaned.count()
    rows_dropped = initial_count - final_count
    
    print(f"Filas eliminadas con nulos: {rows_dropped}")
    print(f"Filas finales: {final_count}\n")
    
    return df_cleaned


# In[ ]:


# Limpiar el DataFrame de pel√≠culas
movies_clean_df = drop_nulls_and_report(movies_transformed_df, "Movies Transformado")

# Limpiar el DataFrame de ratings
ratings_clean_df = drop_nulls_and_report(ratings_transformed_df, "Ratings Transformado")


# ## Agregaci√≥n y Joins simples

# In[ ]:


# --- Agregaciones y Joins Simples ---

# Spark SQL: Contar el n√∫mero total de ratings
total_ratings_sql = spark.sql("SELECT COUNT(*) as total_ratings FROM ratings_view").collect()[0][0]
print(f"Total de ratings en la tabla (SQL): {total_ratings_sql}")

# DataFrame API: Hacer lo mismo
total_ratings_df = ratings_df.count()
print(f"Total de ratings en la tabla (DataFrame API): {total_ratings_df}")

# Spark SQL: Encontrar las 5 pel√≠culas con m√°s ratings
print("\nTop 5 pel√≠culas con m√°s ratings (SQL):")
spark.sql("""
    SELECT
        movieId,
        COUNT(*) as num_ratings
    FROM ratings_view
    GROUP BY movieId
    ORDER BY num_ratings DESC
    LIMIT 5
""").show()

# Join simple con Spark SQL: Unir una pel√≠cula con sus ratings
print("\nJoin simple: Ver los ratings para la pel√≠cula 'Jumanji':")
spark.sql("""
    SELECT
        m.title,
        r.userId,
        r.rating
    FROM movies_view m
    JOIN ratings_view r ON m.movieId = r.movieId
    WHERE m.title = 'Jumanji (1995)'
""").show()


# # Guardar en formato Delta Lake

# ## Uni√≥n de Datos

# In[ ]:


# --- Paso 1: Unir los dos DataFrames en una tabla "Golden" ---
# Usamos un 'inner join' para quedarnos solo con los ratings que tienen una pel√≠cula asociada.

# Es una buena pr√°ctica renombrar las columnas antes del join para evitar ambig√ºedades,
# aunque en este caso 'movieId' es la √∫nica en com√∫n y es la clave del join.
# El join se realizar√° autom√°ticamente en esta columna.

golden_df = ratings_transformed_df.join(
    movies_transformed_df,
    on="movieId",
    how="inner"
)

print("Vista previa de la tabla 'Golden' unida y desnormalizada:")
golden_df.printSchema()
golden_df.show(5, truncate=False)


# ## Guardar versi√≥n final

# In[ ]:


# --- Paso 2: Guardar la Tabla Golden en formato Delta Lake (Corregido) ---

delta_path = "/Volumes/workspace/movie/movielens/delta_tables/movielens_analyzed_final"
# Particionamos por 'year' porque es una columna de baja cardinalidad
# y muy com√∫n para filtrar en an√°lisis de series temporales.
(golden_df.write
 .format("delta")
 .mode("overwrite")
 .partitionBy("year")
 .option("overwriteSchema", "true")
 .save(delta_path))

print(f"‚úÖ Tabla 'Golden' guardada exitosamente en Delta Lake en: {delta_path}")

# --- Validaci√≥n Final ---
print("\nüîç Validaci√≥n: Leyendo la tabla final desde Delta Lake:")
final_df = spark.read.format("delta").load(delta_path)
final_df.show(5)
final_df.printSchema()


# # Visualizaci√≥n

# ## G√©neros mas populares

# In[ ]:


from pyspark.sql.functions import count, avg, desc

# Calcular el n√∫mero de ratings por g√©nero
genre_popularity_df = golden_df.groupBy("genre").agg(count("rating").alias("num_ratings")).orderBy(desc("num_ratings"))

print("Popularidad de los g√©neros por n√∫mero de ratings:")
genre_popularity_df.show()

# --- Visualizaci√≥n ---
display(genre_popularity_df)


# In[ ]:


# Calcular el rating promedio por a√±o
avg_rating_by_year_df = golden_df.filter(col("year").isNotNull()).groupBy("year").agg(avg("rating").alias("avg_rating")).orderBy("year")

print("Rating promedio de las pel√≠culas por a√±o de lanzamiento:")
avg_rating_by_year_df.show(20)

# --- Visualizaci√≥n ---
display(avg_rating_by_year_df)


# # Concusiones
# 
# A partir del an√°lisis del dataset de MovieLens, se pueden extraer las siguientes conclusiones simples:
# 
# 1.  **Dominancia de G√©neros:** Los g√©neros de **Drama** y **Comedia** son los que concentran la mayor cantidad de ratings, lo que sugiere una fuerte preferencia del p√∫blico por este tipo de contenido.
# 
# 2.  **Calidad vs. Antig√ºedad:** El an√°lisis del rating promedio por a√±o muestra que entre las decadas del 20 y el 80 las pel√≠culas tuvieron un rating algo sostenido a diferencia de epocas anteriores y posteriores.
